# 現在の課題

## 精度状況

### v8 (速度変換データ拡張 + GMM 4-mixture)

| テスト条件 | v6 | v7 | v7.1 | v8 (現在) |
|---|---|---|---|---|
| 学習データ（873発話、VOICEVOX 18話者） | 820/873 (93.9%) | 818/873 (93.6%) | 809/873 (92.7%) | 859/873 (98.4%) |
| コーパス内文 × 外部話者 neutral | 10/10 (100%) | — | — | 10/10 (100%) |
| コーパス内文 × 外部話者 感情50% | 9/10 (90%) | — | — | 10/10 (100%) |
| **コーパス外文 × 外部話者 neutral** | **4/10 (40%)** | **8/10 (80%)** | **8/10 (80%)** | **9/10 (90%)** |
| コーパス内文 × 外部話者 感情100% | 2/10 (20%) | — | — | **10/10 (100%)** |

### v8 での改善内容

- **速度変換データ拡張**: 5-way speed perturbation (0.9x, 0.95x, 1.0x, 1.05x, 1.1x)
  - 学習発話: 873 → 4,365 (5倍)
  - ピッチ・話速変動への耐性を獲得
- **GMM mixture増加**: 1 → 4 components per state
  - 3倍以上のデータがあるため4-mixture GMMを安定学習可能に
- **LMウェイト調整**: 23.0 → 20.0

### v8 で解決した失敗例

| 文 | v7.1 結果 | v8 結果 |
|---|---|---|
| 魚と野菜を買う | 魚を直す ✗ | **魚と野菜を買う** ✓ |
| 猫が窓で寝る | 猫が窓で帰る ✗ | **猫が窓で寝る** ✓ |
| 駅から電車に乗る | ✓ | **駅から電車に乗る** ✓ (維持) |

### v7.1 からの継続情報

- **コーパス**: 11,016文（300語彙、工学・ビジネスドメイン含む）
- **LM**: トライグラム（11,639文から構築）
- **辞書**: 899語

#### コーパス拡張時の制約（学習済みルール）

1. **「で」+PLACE展開は厳禁** — 一般場所と専門語の組み合わせで偽トライグラムが発生
2. **「は」+ADJテンプレートは既存カテゴリ（NATURE/THING/PLACE）のみ** — 助詞「は」の過剰挿入を防止
3. **既存動詞リストとの重複を避ける** — 頻度倍増による偽マッチ防止
4. **一般文と音響的に混同しやすい語は除外** — パソコン, 動かす, 壊れる, スイッチ, 地下等

## 残存課題

### 1. 助詞「から」の話者間混同

外部話者話者で「から」(kara) が他の語（部下 buka 等）に置換される。VOICEVOX学習データでは正解。

- 「朝から雨が降る」→「朝部下が雨が降る」

### 2. ~~感情・発話スタイル変動への音響モデル頑健性~~ → v8で解決

感情100%: 2/10 (v6) → **10/10 (v8)**。速度変換データ拡張によりピッチ・話速変動への耐性を獲得。

### 3. 学習データの話者・音源多様性の不足

全学習データがVOICEVOX合成音声のみ。速度変換データ拡張でピッチ・話速の変動は模擬できるが、異なる声質・録音環境は模擬できない。

## 改善方向

| 優先度 | 施策 | 期待効果 |
|---|---|---|
| 高 | 外部話者音声を学習データに追加 | 話者間汎化（「から」等の混同解消） |
| 高 | VTLN（声道長正規化） | 話者正規化による混同低減 |
| 中 | コーパスのさらなる拡張（医療・IT等の新ドメイン） | 未知文カバレッジ向上 |
| ~~低~~ | ~~感情パラメータ付きデータの追加学習~~ | ~~感情変動への耐性~~ → v8で解決済み |

## ~~パフォーマンス最適化~~ → 全9件対応済み

### 特徴量抽出: Extract **-48%**, allocs **-99.3%**

| ベンチマーク | Before | After | 速度 | メモリ | allocs |
|---|---|---|---|---|---|
| Extract_1sec | 1,150,000 ns / 574 allocs | 597,000 ns / 104 allocs | **-48%** | **-50%** | **-82%** |
| Extract_5sec | 5,800,000 ns / 2,574 allocs | 3,020,000 ns / 104 allocs | **-48%** | **-58%** | **-96%** |
| Extract_30sec | 34,470,000 ns / 15,074 allocs | 17,807,000 ns / 104 allocs | **-48%** | **-61%** | **-99.3%** |

| # | 施策 | 状態 |
|---|---|---|
| 1 | Hamming窓係数テーブル事前計算 | ✅ |
| 2 | FFT twiddle factorテーブル事前計算 | ✅ |
| 3 | cepstraバッファ一括確保 | ✅ |
| 4 | deltaバッファ一括確保 | ✅ |
| 5 | bit-reversal permutationテーブル化 | ✅ |
| 6 | フレーム分割をslice viewに変更、窓適用をFFTバッファロードに融合 | ✅ |

### 学習: BaumWelch **-15%**

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| BaumWelch_10seq_50frames | 2,050,000 ns | 1,744,000 ns | **-15%** |

| # | 施策 | 状態 |
|---|---|---|
| 7 | E-step統計量蓄積のポインタチェイシング削減・乗算最適化 | ✅ |

### デコーダ: Decode **-17%**

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| Decode_5vocab_50frames | 1,044,000 ns | 909,000 ns | **-13%** |
| Decode_10vocab_100frames | 6,068,000 ns | 5,365,000 ns | **-12%** |
| Decode_20vocab_200frames | 44,122,000 ns | 36,428,000 ns | **-17%** |

| # | 施策 | 状態 |
|---|---|---|
| 8 | トークン再結合キーをstring→数値IDに変更 | ✅ |
| 9 | ビームプルーニングをsort.Slice→quickselectに変更 | ✅ |

## 現在の推奨構成

```
AM:   data/work/am_tri_aug5_mix4.gob (5-way speed perturbation, 4-mix GMM)
LM:   data/work/lm_v7.arpa (トライグラム、11,639文から構築)
Dict: data/work/dict_lm_v7.txt (899語)
Flags: -oov-prob -5.0 -lm-weight 20.0 -max-tokens 5000

学習コマンド:
go run ./cmd/train \
  -manifest data/manifest_all_v3.tsv \
  -dict data/work/dict_lm_v7.txt \
  -output data/work/am_tri_aug5_mix4.gob \
  -mix 4 -iter 20 -align-iter 2 -triphone -augment
```
