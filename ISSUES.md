# 現在の課題

## 精度状況

### v10 (トライグラムLM + Wikipedia NLP + デコーダ再結合拡張)

| テスト条件 | v8 | v10-bigram | v10-trigram (現在) |
|---|---|---|---|
| 学習データ (2,469発話 × 5-way augment, TTS 30話者) | 859/873 (98.4%) | — | — |
| コーパス内文 × 外部TTS話者 (test_tts, 10文) | 10/10 (100%) | 8/10 (80%) | **1/10 (10%)** ⚠️ |
| コーパス内文 × 外部話者 (test_voicepeak, 10文) | — | — | **9/10 (90%)** |
| 感情音声 (test_voicepeak_emotion, 10文) | — | — | **9/10 (90%)** |
| コーパス外文 × 未知TTS話者A (20文) | — | 8/20 (40%) | **10/20 (50%)** |
| コーパス外文 × 未知TTS話者B (20文) | — | 8/20 (40%) | **7/20 (35%)** |
| コーパス外文 × 未知TTS話者C (20文) | — | 10/20 (50%) | **11/20 (55%)** |
| **コーパス外 3話者平均** | — | **8.7/20 (43%)** | **9.3/20 (47%)** |

### v10-trigram での変更内容 (v10-bigramからの差分)

- **トライグラムLM**: テンプレート14,250文 + Wikipedia抽出393文 → 576 unigrams, 3,633 bigrams, 15,357 trigrams
- **Wikipedia自然言語テキストフィルタ**: `cmd/wikitext` でMediaWiki XMLダンプから文抽出 → `cmd/lmtext` でMeCab分かち書き+辞書フィルタ (4,173,332文 → 393文、採用率0.03%)
- **デコーダ再結合キー拡張**: `recomKey` に `prevWordID` 追加。1語履歴 → 2語履歴でトライグラムの効果を活用
- **LM構築**: `lmbuild -order 3` でWitten-Bellスムージング付きトライグラム

### v10-trigram の評価分析

**test_ttsリグレッション** (80% → 10%): 短い発話 (2〜4語) で深刻な誤認識が多発。

| 正解 | 認識結果 | 分析 |
|---|---|---|
| お茶 を 飲む | 課長 が 部 | 1語目から崩壊 |
| 花 が 咲く | 体 が 遊ぶ 使う | 過挿入 |
| 犬 と 散歩 する | 犬 が 検討 する | 助詞+名詞の連鎖ミス |
| 音楽 を 聴く | 音楽 を 受ける | 動詞混同 |

test_voicepeakは同じコーパス内文で9/10 — test_ttsリグレッションは特定話者の音響特性とトライグラムLMの相互作用が原因の可能性。

**3話者共通エラー** (コーパス外文):
| 正解 | 認識結果 | 全3話者で共通 |
|---|---|---|
| 忘れ物 を **取り** に 行く | 忘れ物 を **撮り** に 行く | ✓ (同音異義語) |
| 傘 を 持つ | 不安定 (脱落・置換) | ✓ (短発話) |
| 薬 を 飲む | 水/学生 を/も 飲む | ✓ (1語目混同) |

### v10-bigram での変更内容 (v8からの差分)

- **デコーダ出力バグ修正**: `strings.Join(words, "")` → `strings.Join(words, " ")` — 単語間スペースが欠落していた
- **学習データ話者追加**: 18話者 → 30話者 (873 → 2,469発話、augment後 12,345発話)
- **コーパス拡張**: 11,016文 → 14,250文 (14テンプレート追加、325語彙)
  - 日常生活パターン: 「PERSON が THING を VERB」「BODY が VERB_I」「THING を V-stem に VERB_MOTION」等
  - na形容詞述語: 「NATURE が きれい です」
- **LM再構築**: lm_v8.arpa (14,383文から構築、バイグラム)
- **辞書拡張**: 899語 → 1,200語
  - 追加語彙: 元気, 空く, 着る, 決める, お腹, 準備, 忘れ物, 食材, 目標, 午前, 午後 等
- **辞書サイズ調整**: 4,000語で音響混同が悪化 → 1,200語に絞り込み

### v10 で観察された誤認識パターン

| 種別 | 例 | 原因 |
|---|---|---|
| 同音異義語 | 取り → 撮り, 着る → 切る | AM弁別力不足 (LMでも解決困難) |
| 音響類似語 | 雨 → 姉, 食材 → 食堂, 薬 → 水/学生 | 短母音の混同 |
| 助詞脱落/置換 | 「と」→「が」, 「から」→脱落 | 話者依存の音響変動 |
| 短発話不安定 | 2-3語文での誤認識率が高い | LMコンテキスト不足+AM不確実性 |
| 過挿入 | 「花 が 咲く」→「体 が 遊ぶ 使う」 | トライグラムLMが短文で過制約 |

### v8 からの継続情報

- **速度変換データ拡張**: 5-way speed perturbation (0.9x, 0.95x, 1.0x, 1.05x, 1.1x)
- **GMM**: 4-mixture per state

#### コーパス拡張時の制約（学習済みルール）

1. **「で」+PLACE展開は厳禁** — 一般場所と専門語の組み合わせで偽トライグラムが発生
2. **「は」+ADJテンプレートは既存カテゴリ（NATURE/THING/PLACE）のみ** — 助詞「は」の過剰挿入を防止
3. **既存動詞リストとの重複を避ける** — 頻度倍増による偽マッチ防止
4. **一般文と音響的に混同しやすい語は除外** — パソコン, 動かす, 壊れる, スイッチ, 地下等
5. **辞書サイズは1,200語程度が最適** — 4,000語では音響混同が増加、899語ではカバレッジ不足

## 残存課題

### 1. test_ttsリグレッション (80% → 10%) ⚠️

トライグラムLM導入でtest_ttsが大幅に悪化。test_voicepeakは90%で良好なため、特定話者との相性問題の可能性あり。バイグラムLMへの切り戻し、またはLM interpolation weight調整で改善できる可能性。

### 2. コーパス外文の認識精度 (47%)

20文 × 3未知話者で平均9.3/20。v10-bigramの43%から微増したが依然として低い。

### 3. 音響的に類似した単語の弁別

同音異義語（取り/撮り — 全3話者で共通ミス）や音響類似語（雨/姉）の混同。AMのGMM表現力の限界。

### 4. 学習データの話者・音源多様性の不足

全学習データがTTS合成音声（30話者）のみ。実音声の声質・録音環境は模擬できない。

### 5. ~~感情・発話スタイル変動への音響モデル頑健性~~ → v8で解決

感情100%: 2/10 (v6) → **10/10 (v8)**。速度変換データ拡張によりピッチ・話速変動への耐性を獲得。

## 改善方向

| 優先度 | 施策 | 期待効果 |
|---|---|---|
| 高 | test_ttsリグレッション調査 (LM weight調整, bigram/trigram比較) | test_tts精度の回復 |
| 高 | 実音声データの追加学習 | 話者間汎化の向上 |
| 高 | VTLN（声道長正規化） | 話者正規化による混同低減 |
| 中 | 辞書の音響類似語フィルタリング強化 | 同音異義語・類似語の混同低減 |
| 低 | コーパスのドメイン拡張（医療・IT等） | 未知文カバレッジ向上 |

### ~~自然言語テキストによるLM強化~~ → 実施済み

**実施内容**:
1. `cmd/wikitext`: MediaWiki XMLダンプから文抽出 (Go実装、bz2直接パース)
2. `cmd/lmtext`: MeCab分かち書き + 辞書フィルタ (バッチ処理、`-min-words 3`)
3. Wikipedia日本語ダンプ100Kページ → 4,173,332文 → 辞書フィルタ後393文
4. テンプレート14,250文 + NLP 393文 → トライグラムLM構築
5. デコーダ `recomKey` に `prevWordID` 追加 (トライグラム再結合)

**結果**: コーパス外3話者平均が43% → 47%に微増。ただしtest_ttsに深刻なリグレッション (80% → 10%) が発生。Wikipedia由来の393文では自然言語パターンの補強効果が限定的。辞書1,176語でのフィルタ採用率が0.03%と極めて低く、テキスト量の問題よりも辞書カバレッジがボトルネック。

## ~~パフォーマンス最適化~~ → 全10件対応済み

### 特徴量抽出: Extract **-48%**, allocs **-99.3%**

| ベンチマーク | Before | After | 速度 | メモリ | allocs |
|---|---|---|---|---|---|
| Extract_1sec | 1,150,000 ns / 574 allocs | 597,000 ns / 104 allocs | **-48%** | **-50%** | **-82%** |
| Extract_5sec | 5,800,000 ns / 2,574 allocs | 3,020,000 ns / 104 allocs | **-48%** | **-58%** | **-96%** |
| Extract_30sec | 34,470,000 ns / 15,074 allocs | 17,807,000 ns / 104 allocs | **-48%** | **-61%** | **-99.3%** |

| # | 施策 | 状態 |
|---|---|---|
| 1 | Hamming窓係数テーブル事前計算 | ✅ |
| 2 | FFT twiddle factorテーブル事前計算 | ✅ |
| 3 | cepstraバッファ一括確保 | ✅ |
| 4 | deltaバッファ一括確保 | ✅ |
| 5 | bit-reversal permutationテーブル化 | ✅ |
| 6 | フレーム分割をslice viewに変更、窓適用をFFTバッファロードに融合 | ✅ |

### 学習: BaumWelch **-15%**

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| BaumWelch_10seq_50frames | 2,050,000 ns | 1,744,000 ns | **-15%** |

| # | 施策 | 状態 |
|---|---|---|
| 7 | E-step統計量蓄積のポインタチェイシング削減・乗算最適化 | ✅ |

### デコーダ: Decode **-17%**

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| Decode_5vocab_50frames | 1,044,000 ns | 909,000 ns | **-13%** |
| Decode_10vocab_100frames | 6,068,000 ns | 5,365,000 ns | **-12%** |
| Decode_20vocab_200frames | 44,122,000 ns | 36,428,000 ns | **-17%** |

| # | 施策 | 状態 |
|---|---|---|
| 8 | トークン再結合キーをstring→数値IDに変更 | ✅ |
| 9 | ビームプルーニングをsort.Slice→quickselectに変更 | ✅ |

### 学習パイプライン: 全体 **-77%** (4.4x高速化)

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| 全学習パイプライン (2,469発話, 5-way augment, triphone) | 4m33s | 1m02s | **-77%** |

| # | 施策 | 状態 |
|---|---|---|
| 10 | 特徴量抽出・モノフォン学習・強制アライメント・トライフォン学習をgoroutineワーカプール(NumCPU)で並列化 | ✅ |

## 現在の推奨構成

```
AM:   models/v10/am.gob (30話者 2,469発話, 5-way augment, 4-mix GMM, トライフォン)
LM:   models/v10/lm.arpa (トライグラム、14,643文から構築: テンプレート14,250 + Wikipedia393)
Dict: models/v10/dict.txt (1,176語)
Flags: -oov-prob -5.0 -lm-weight 10.0 -max-tokens 5000

認識コマンド:
/tmp/transcript \
  -am models/v10/am.gob \
  -lm models/v10/lm.arpa \
  -dict models/v10/dict.txt \
  -wav input.wav \
  -oov-prob -5.0 -lm-weight 10.0 -max-tokens 5000

学習コマンド:
go run ./cmd/train \
  -manifest data/manifest_all_v4.tsv \
  -dict data/work/dict_lm_v8.txt \
  -output data/work/am_tri_v10.gob \
  -mix 4 -iter 20 -align-iter 2 -triphone -augment

LM構築コマンド:
go run ./cmd/lmbuild -order 3 -output models/v10/lm.arpa \
  training/corpus8_expanded.txt data/work/nlp_corpus.txt
```
