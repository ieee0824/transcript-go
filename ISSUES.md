# 現在の課題

## 精度状況

### v11 (現在)

| テスト条件 | 精度 | v10比 |
|---|---|---|
| コーパス内文 × 外部話者 (test_external, 10文) | **9/10 (90%)** | ±0 |
| 感情音声 (test_emotion, 10文) | **9/10 (90%)** | ±0 |
| コーパス外文 × 未知TTS話者A (20文) | **12/20 (60%)** | +1 |
| コーパス外文 × 未知TTS話者B (20文) | **11/20 (55%)** | +4 |
| コーパス外文 × 未知TTS話者C (20文) | **11/20 (55%)** | ±0 |
| **コーパス外 3話者平均** | **34/60 (57%)** | **+10pt** |
| コーパス内文 × 特定TTS話者 (test_tts, 10文) | **5/10 (50%)** | +4 |

### v11 での変更内容

- **TTS話者追加**: 30話者 → 55話者 (+25話者、全てTTS)
- **学習データ増量**: 2,469発話 → 5,794発話 (augment後 28,970発話)
- **トライフォン**: 304個
- LM・辞書はv10と同一

### v10

#### VTLN評価結果

認識時VTLNグリッドサーチ (α ∈ {0.82, 0.84, ..., 1.20}) を実装・評価。FFTパワースペクトルは1回のみ計算し、メルフィルタバンク以降をα毎に再計算。スコアリングはモノフォンGMMのフレーム最大尤度。

| テスト条件 | VTLN無し | VTLN有り | 変化 |
|---|---|---|---|
| コーパス内文 × 外部話者 (test_external) | 9/10 | 9/10 | ±0 |
| 感情音声 (test_emotion) | 9/10 | 9/10 | ±0 |
| コーパス内文 × 特定TTS話者 (test_tts) | 1/10 | 2/10 | +1 |
| コーパス外文 × 話者A | 11/20 | 10/20 | -1 |
| コーパス外文 × 話者B | 7/20 | 7/20 | ±0 |
| コーパス外文 × 話者C | 10/20 | 10/20 | ±0 |
| **コーパス外 3話者平均** | **28/60 (46.7%)** | **27/60 (45.0%)** | **-1.7%** |

効果はほぼ中立。TTS合成音声は話者間の声道長差が実音声より小さく、VTLNで補正すべき変動が限定的と考えられる。

#### test_ttsについて

test_tts (1/10) は切り分け調査の結果、トライグラム変更によるリグレッションではないことが判明。v10-bigramバイナリ (commit 91a8549) + 旧バイグラムLM + 旧デコーダでも2/10であり、ISSUES.mdに記録されていた8/10は再現不能。test_externalが同じコーパス内文で9/10であることから、test_tts話者固有の音響ミスマッチが原因。

| 切り分け条件 | LM | デコーダ | test_tts |
|---|---|---|---|
| v10-bigramバイナリ (commit 91a8549) | バイグラム | 旧 | 2/10 |
| バイグラムLM + 旧デコーダ相当 | バイグラム | prevWordID=0 | 2/10 |
| バイグラムLM + 新デコーダ | バイグラム | prevWordIDあり | 2/10 |
| トライグラムLM + 旧デコーダ相当 | トライグラム | prevWordID=0 | 1/10 |
| トライグラムLM + 新デコーダ (現在) | トライグラム | prevWordIDあり | 1/10 |

### 辞書拡張 + GMM分解能実験 (v13系)

辞書を1,176語→2,000語に拡張し、さらにGMM混合数の増加を試行。

#### 辞書拡張

JSUT + Common Voiceコーパス (288,173文) をMeCab分かち書きし、未カバー語の出現頻度上位824語を追加。MeCab読み → KanaToPhonemes で音素列を自動生成。

| 辞書 | 語数 | JSUT通過 | CV通過 |
|---|---|---|---|
| v11 (ベース) | 1,176 | 4/7,696 (0.05%) | 4,686/280,477 (1.7%) |
| 拡張 (未フィルタ) | 2,024 | 154 (38x) | 34,390 (7.3x) |
| 拡張 (混同フィルタ済) | 1,694 | - | - |

混同フィルタ: 既存辞書語との音素編集距離≤1かつ出現頻度<3,000の語を除外 (330語除外)。

#### 評価結果

| テスト条件 | v11 (4mix, 1,176語) | 拡張辞書 4mix (1,694語) | 拡張辞書 8mix (1,694語) |
|---|---|---|---|
| 外部話者 IC | 9/10 → **10/10** | **10/10** | 8/10 |
| 感情 IC | 9/10 → **10/10** | **10/10** | 9/10 |
| **IC合計** | **18/20** → **20/20** | **20/20** | **17/20** |
| コーパス外 3話者 | **34/60 (57%)** | **33/60 (55%)** | **23/60 (38%)** |

#### 分析

- **辞書拡張 (4-mix)**: 混同フィルタにより IC は完璧 (20/20) を維持。OOC は -2pt (34→33) とほぼ横ばい
- **8-mix GMM**: 過学習が発生。304トライフォン × 3状態 × 8混合 = 7,296ガウス分布はデータ量に対してパラメータ過多。ICさえ悪化 (20→17) したことから、ノイズを学習している
- **結論**: 4-mix GMMが現在のデータ量 (28,970発話) での最適点。GMM分解能向上にはデータ3-5倍増強、またはDNN-HMMへの移行が必要

### DNN-HMM ハイブリッド実験

音響尤度計算のみをDNNに差し替え (DNN-HMM hybrid)。デコーダ (Viterbi beam search)、LM、辞書はすべてそのまま。emitCacheの計算方法だけ変更。

#### DNN仕様

| パラメータ | 値 |
|---|---|
| アーキテクチャ | MLP 3層 (429→256 ReLU→256 ReLU→87 log-softmax) |
| 入力 | ±5フレームコンテキスト窓 × 39次元MFCC = 429次元 |
| 出力 | 87クラス (29音素 × 3発話状態) |
| パラメータ数 | ~196K |
| 学習データ | 28,970発話 (5-way augment), 3,309,827フレーム |
| アライメント | am_tri_8mix.gob によるGMM強制アライメント |
| 最適化 | Adam (lr=0.001, batch=256), early stopping patience=3 |
| 結果 | val_acc 69.9%, 8エポックで収束 |

#### 評価結果（同一コード・デコーダパラメータで統一比較）

| 構成 | 辞書/LM | IC | OOC |
|---|---|---|---|
| v11 4-mix GMM | v11 (1,176語) | 17/20 | 26/60 (43%) |
| v11 4-mix GMM | filtered (1,694語) | 17/20 | 25/60 (42%) |
| 8-mix GMM | filtered (1,694語) | 17/20 | 23/60 (38%) |
| **DNN-HMM** | **v11 (1,176語)** | **17/20** | **28/60 (47%)** |
| **DNN-HMM** | **filtered (1,694語)** | **19/20** | **27/60 (45%)** |

#### 分析

- **DNN-HMM vs GMM**: 全構成でOOC +2〜4発話の改善。filtered辞書ではIC +2も改善
- **最良構成**: DNN-HMM + v11辞書で OOC 28/60 (47%)、DNN-HMM + filtered辞書で IC 19/20
- **DNN改善の性質**: 短発話の安定化 (「山 まで 歩く」等)、音響弁別力の向上 (「本」vs「パン」は依然困難)
- **val_acc 69.9%の限界**: フレームレベル精度がまだ低く、改善余地あり。より深いネットワーク、より多くの学習データ、またはBatchNorm/Dropout等で改善可能
- **AM非依存**: v11 AM でも 8-mix AM でも DNN-HMM の認識結果は同一 (DNN が emission を完全に置き換えるため、GMM は無関係)
- **備考**: v13セクションのGMM評価値と本セクションの値が異なるのは、v13評価後のコード変更 (LogZero修正等) による再計測結果
- **学習高速化**: 並列サブバッチ処理 (NumCPU個のワーカーで同時にforward/backward、gradient accumulation) により学習速度2.2x向上 (1,112ms→512ms/epoch, 100Kフレーム, M2)

### DNN深層化 + Dropout実験 (v14)

DNNアーキテクチャを可変層対応にリファクタリングし、より深いネットワーク + Dropoutで精度改善を試行。

#### アーキテクチャ変更

| | v13 (旧) | v14 (新) |
|---|---|---|
| 隠れ層 | 2層 × 256 | 4層 × 512 |
| Dropout | なし | 0.2 (inverted) |
| パラメータ数 | ~196K | ~1,052K (5.4倍) |
| 入力/出力 | 429→87 | 429→87 (同一) |

実装: `DNNLayer`構造体導入、`[]DNNLayer`スライスによるN層ループ化、inverted dropout (学習時のみ、ワーカーごと独立RNG)。シリアライズV2形式 (V1後方互換読み込み)。`-layers`, `-dropout` CLIフラグ追加。

#### 学習結果

| | v13 | v14 |
|---|---|---|
| エポック数 | 8 (patience=3) | 50 |
| val_acc (フレーム) | 69.9% | **80.0%** (+10.1pt) |
| val_loss | - | 0.5385 |

#### 評価結果

| テスト条件 | v13 DNN-HMM (2層×256) | v14 DNN-HMM (4層×512+DO) | 差 |
|---|---|---|---|
| コーパス内文 × 外部話者 (test_external) | 9/10 | 8/10 | -1 |
| 感情音声 (test_emotion) | 10/10 | 10/10 | ±0 |
| 特定TTS話者 (test_tts) | 7/10 | 7/10 | ±0 |
| **IC合計** | **26/30 (87%)** | **25/30 (83%)** | **-1** |
| コーパス外文 × 話者A | 10/20 | 10/20 | ±0 |
| コーパス外文 × 話者B | 9/20 | 9/20 | ±0 |
| コーパス外文 × 話者C | 8/20 | **11/20** | **+3** |
| **OOC合計** | **27/60 (45%)** | **30/60 (50%)** | **+5pt** |

#### 分析

- **フレーム精度**: val_acc 69.9%→80.0% (+10.1pt)。Dropoutにより50エポックまでval_lossが減少し続けた (旧モデルは8エポックで収束)
- **OOC認識**: 27/60→30/60 (+5pt)。特に話者Cで+3発話改善。フレーム精度向上が汎化性能に寄与
- **IC微減**: 26/30→25/30 (-1)。外部話者テストで1文悪化 (「朝 から 雨 が 降る」→「朝 ごはん を 描く する」)。デコーダパラメータ調整では改善せず、音響レベルの混同
- **結論**: 深層化+Dropoutはフレーム精度・OOC認識精度の両方で有効。ICの微減は統計的揺らぎの範囲。OOC 50%は本プロジェクトのDNN-HMM最高値

### Cosine LR Annealing実験 (v15)

v14アーキテクチャ (4層×512, Dropout 0.2) に学習率スケジューリングを追加。Label Smoothingも実装・評価。

#### 実装

- **Cosine LR**: `lr_t = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(π * epoch / maxEpochs))`、lr_min = lr_max × 0.01
- **Label Smoothing**: `y_smooth[j] = (1-ε) * one_hot[j] + ε/K` (K=87クラス)
- CLIフラグ: `-lr-schedule cosine`, `-label-smooth 0.1`

#### 学習結果

| 構成 | val_acc | val_loss | 備考 |
|---|---|---|---|
| v14 (constant LR) | 80.0% | 0.5385 | ベースライン |
| cosine LR + label-smooth ε=0.1 | 83.7% | 0.5293 | デコーダ認識不可 (出力logit平坦化) |
| cosine LR + label-smooth ε=0.05 | 83.9% | 0.4697 | デコーダ認識不可 (同上) |
| **cosine LR のみ** | **84.0%** | **0.4196** | **採用** |

Label Smoothingはε=0.1/0.05いずれもデコーダとの互換性を失う。出力層の重みが縮小しsoftmax出力が均一化、beam searchで有意な音響スコア差が出ず認識結果が空になる。フレーム精度は高い (83.7-83.9%) がエンドツーエンドの認識性能には寄与しない。

#### 評価結果

| テスト条件 | v14 (constant LR) | v15 (cosine LR) | 差 |
|---|---|---|---|
| コーパス内文 × 外部話者 (test_external) | 8/10 | **9/10** | **+1** |
| 感情音声 (test_emotion) | 10/10 | 10/10 | ±0 |
| 特定TTS話者 (test_tts) | 7/10 | 7/10 | ±0 |
| **IC合計** | **25/30 (83%)** | **26/30 (87%)** | **+1** |
| コーパス外文 × 話者A | 10/20 | 10/20 | ±0 |
| コーパス外文 × 話者B | 9/20 | 11/20 | **+2** |
| コーパス外文 × 話者C | 11/20 | **15/20** | **+4** |
| **OOC合計** | **30/60 (50%)** | **36/60 (60%)** | **+10pt** |

#### 分析

- **フレーム精度**: val_acc 80.0%→84.0% (+4.0pt)。Cosine LRが後半エポックで学習率を緩やかに減衰させ、v14では到達できなかった最適解に収束
- **OOC認識**: 30/60→36/60 (+10pt)。特に話者Cで+4発話、話者Bで+2発話改善。フレーム精度の向上が音響弁別力に直結
- **IC認識**: v14で失っていた「朝 から 雨 が 降る」(test_external) がv15で復活
- **Label Smoothingの教訓**: ε=0.05/0.1いずれもデコーダとの互換性を失う。87クラス分類でsoftmax出力が均一化し、beam searchが機能しない。HMM-DNNハイブリッド系ではlabel smoothingは非推奨。推論時temperature scalingも検討したが、根本的にlogitのダイナミックレンジが不足

### 辞書拡張 + Wikipedia LM実験 (v16)

v15 (1,694語) を拡張し外来語カバレッジを向上させる実験。KanaToPhonemes外来語バグ修正 + 音素編集距離フィルタ + Wikipedia LM構築。

#### KanaToPhonemes外来語バグ修正

lexicon/kana.go に27種の外来語拗音マッピング (チェ→ch e, シェ→sh e, ウェ→u e等) + 小文字カナフォールバック (ァ,ィ,ゥ,ェ,ォ) を追加。dictfixツールで全dict.txtを再生成、65,865エントリの音素列を修正。

#### 辞書サイズ実験

| 構成 | 語数 | LM | IC | OOC | 合計 |
|---|---|---|---|---|---|
| v15 (baseline) | 1,694 | v15 LM (14K文) | 26/30 | 36/60 | **62/90 (68.9%)** |
| v15辞書 + 新LM | 1,694 | 新LM (30K文) | 25/30 | 36/60 | 61/90 (67.8%) |
| 1,745 + v15 LM | 1,745 | v15 LM | 25/30 | 36/60 | 61/90 (67.8%) |
| 2,000 + v15 LM | 2,000 | v15 LM | 25/30 | 36/60 | 61/90 (67.8%) |
| **2,000 + 新LM (v16)** | **2,000** | **新LM (30K文)** | **24/30** | **37/60** | **61/90 (67.8%)** |
| 1,745 + 新LM | 1,745 | 新LM (30K文) | 24/30 | 36/60 | 60/90 (66.7%) |
| 5,000 + 新LM | 5,000 | 新LM (30K文) | 17/30 | 32/60 | 49/90 (54.4%) |

v16辞書構成: must-include 1,694 + コーパス語 51 (外来語含む) + fill 255 = 2,000語
新LM: corpusgen (18,367文) + Wikipedia (14.8M文中11,407文通過) = 29,774文からトライグラム構築

#### 分析

- **5,000語は過剰**: 49/90 (-13)。音響混同が激増。v10の知見 (4,000語で悪化) と一致
- **2,000語が最適点**: v15と-1差で外来語51語を追加。IC -2 / OOC +1
- **LM変更自体が-1**: v15辞書のまま新LMに変えただけで61/90。コーパステンプレート拡張による不自然なn-gramが原因
- **辞書拡張の効果**: v15 LMのまま辞書だけ拡張しても-1 (パソコン等の新語が既存語と音響混同)
- **混同フィルタ**: 音素編集距離≥2で43K語を除外。≥3では主要外来語も除外されるため≥2が適切
- **コーパス語優先**: corpusgen外来語テンプレートで生成したコーパスの語を fill より優先して辞書に含める方式を採用

追加された主要外来語: マイク, コーヒー, テレビ, エアコン, カメラ, コンビニ, スーパー, デパート, ホテル, パソコン, チェック, サッカー, テニス 等

### デコーダパラメータチューニング

`cmd/tuner` グリッドサーチツールを作成し、LMWeight / WordInsertionPenalty / MaxActiveTokens の最適値を語彙サイズ別に探索。デフォルト (LW=10, WP=0, MT=1000) は大幅に非最適だったことが判明。

#### Word-End Pruning

`decoder/viterbi.go` に `MaxWordEnds` パラメータを追加。語末完了をバッファリングし、フレームあたりの展開数を制限。ただし今回のテストでは MaxActiveTokens 増加のほうが効果的で、MaxWordEnds による改善は見られなかった。

#### パラメータ最適化結果

| 構成 | 語数 | パラメータ | IC | OOC | 合計 |
|---|---|---|---|---|---|
| v15旧デフォルト | 1,694 | LW=10, WP=0, MT=1000 | 26/30 | 36/60 | 62/90 (68.9%) |
| **v15チューニング済** | **1,694** | **LW=11, WP=13, MT=1000** | **28/30** | **44/60** | **72/90 (80.0%)** |
| v16旧デフォルト | 2,000 | LW=10, WP=0, MT=1000 | 24/30 | 37/60 | 61/90 (67.8%) |
| **v16チューニング済** | **2,000** | **LW=11, WP=5, MT=2000** | — | — | **70/90 (77.8%)** |
| 5,000旧デフォルト | 5,000 | LW=10, WP=0, MT=1000 | 22/30 | 27/60 | 49/90 (54.4%) |
| **5,000チューニング済** | **5,000** | **LW=10, WP=5, MT=3000** | **25/30** | **45/60** | **70/90 (77.8%)** |

#### 分析

- **WordInsertionPenaltyが最重要**: WP=0→5で全語彙サイズで+5〜+8。WP=13でv15は+10。旧デフォルトWP=0ではデコーダが語境界を過少検出していた
- **MaxActiveTokensは語彙に比例**: 1,694語ではMT=1000で十分。5,000語ではMT=3000が必要 (MT=1000では正解仮説がビームから脱落)
- **LMWeight=10〜11が最適**: 旧デフォルトの10で概ね正しいが、11がわずかに良い
- **5,000語の「壊滅的劣化」はパラメータ起因**: MT=3000+WP=5で70/90に回復。辞書拡張自体は問題なく、ビーム幅不足が原因だった
- **MaxWordEndsは現状不要**: テスト範囲ではMaxActiveTokens増加で十分。10,000語超の場合に有効な可能性

### Common Voice実音声混合訓練実験 (v12/v12b)

Mozilla Common Voice日本語コーパス (CC-0) の実音声を追加学習データとして取り込む実験を実施。辞書1,176語フィルタにより280,477発話中4,686発話が通過 (1.7%)。

#### 実験条件

| モデル | TTS (55話者) | Common Voice (4,686発話) | トライフォン |
|---|---|---|---|
| v11 (ベースライン) | 5,794発話, 5-way augment | なし | 304 |
| v12 (全augment) | 5,794発話, 5-way augment | 4,686発話, 5-way augment | 808 |
| v12b (CV noaug) | 5,794発話, 5-way augment | 4,686発話, augmentなし | 684 |

#### 結果

| テスト条件 | v11 | v12 (全aug) | v12b (CV noaug) |
|---|---|---|---|
| コーパス内文 × 外部話者 (test_external) | 9/10 | 8/10 | **10/10** |
| 感情音声 (test_emotion) | 9/10 | 7/10 | 9/10 |
| 特定TTS話者 (test_tts) | 5/10 | 7/10 | **8/10** |
| コーパス外文 × 話者A | 12/20 | 4/20 | 4/20 |
| コーパス外文 × 話者B | 11/20 | 7/20 | 7/20 |
| コーパス外文 × 話者C | 11/20 | 9/20 | 10/20 |
| **コーパス外 3話者平均** | **34/60 (57%)** | **20/60 (33%)** | **21/60 (35%)** |

#### 分析

- コーパス内文・特定話者では改善 (test_tts: 5/10→8/10)
- コーパス外文では大幅なリグレッション (57%→33-35%)
- **原因**: 4-mix GMMではTTS合成音声と実音声の異なる音響分布を同時にモデル化できない。実音声追加によりGMM分布が拡散し、弁別力が低下
- **結論**: HMM-GMMアーキテクチャの表現力限界。実音声活用にはDNN-HMMやend-to-endへの移行が必要
- v11 (TTS only) が現行アーキテクチャでの最良モデル

### v11 3話者共通エラー (コーパス外文)

| 正解 | 認識結果 | 分析 |
|---|---|---|
| 忘れ物 を **取り** に 行く | 忘れ物 を **撮り** に 行く | 同音異義語 (全3話者共通、v10から継続) |
| 傘 を 持つ | 不安定 (脱落・置換) | 短発話 (全3話者共通、v10から継続) |
| お腹 が 空く | 脱落 | 短発話 (2/3話者、v11で新規) |
| 赤い 花 が 咲く | 高い/北海道 花 が 咲く | 形容詞混同 (2/3話者) |
| 午後 から 仕事 を する | 不安定 (挿入・置換) | 全3話者で誤認識 |
| 食材 を 買い に 行く | 宿題/食堂 を 買い に 行く | 1語目混同 (全3話者共通) |

### v10 での変更内容 (v8からの累積)

- **デコーダ出力バグ修正**: `strings.Join(words, "")` → `strings.Join(words, " ")`
- **学習データ話者追加**: 18話者 → 30話者 (873 → 2,469発話、augment後 12,345発話)
- **コーパス拡張**: 11,016文 → 14,250文 (14テンプレート追加、325語彙)
- **辞書拡張**: 899語 → 1,176語 (辞書サイズ4,000語では音響混同が増加、1,200語程度が最適)
- **トライグラムLM**: テンプレート14,250文 + Wikipedia抽出393文 → Witten-Bellスムージング付きトライグラム
- **Wikipedia NLPパイプライン**: `cmd/wikitext` (MediaWiki XMLダンプ文抽出) + `cmd/lmtext` (MeCab分かち書き+辞書フィルタ)
- **デコーダ再結合キー拡張**: `recomKey` に `prevWordID` 追加 (2語履歴でトライグラム活用)

### v10 で観察された誤認識パターン

| 種別 | 例 | 原因 |
|---|---|---|
| 同音異義語 | 取り → 撮り, 着る → 切る | AM弁別力不足 (LMでも解決困難) |
| 音響類似語 | 雨 → 姉, 食材 → 食堂, 薬 → 水/学生 | 短母音の混同 |
| 助詞脱落/置換 | 「と」→「が」, 「から」→脱落 | 話者依存の音響変動 |
| 短発話不安定 | 2-3語文での誤認識率が高い | LMコンテキスト不足+AM不確実性 |

### v8 からの継続情報

- **速度変換データ拡張**: 5-way speed perturbation (0.9x, 0.95x, 1.0x, 1.05x, 1.1x)
- **GMM**: 4-mixture per state

#### コーパス拡張時の制約（学習済みルール）

1. **「で」+PLACE展開は厳禁** — 一般場所と専門語の組み合わせで偽トライグラムが発生
2. **「は」+ADJテンプレートは既存カテゴリ（NATURE/THING/PLACE）のみ** — 助詞「は」の過剰挿入を防止
3. **既存動詞リストとの重複を避ける** — 頻度倍増による偽マッチ防止
4. **一般文と音響的に混同しやすい語は除外** — パソコン, 動かす, 壊れる, スイッチ, 地下等
5. **辞書サイズは1,200語程度が最適** — 4,000語では音響混同が増加、899語ではカバレッジ不足

## 残存課題

### 1. コーパス外文の認識精度 (57%)

20文 × 3未知話者で平均34/60。v10の47%から+10pt改善したが、依然として辞書1,176語のカバレッジ限界が残る。

### 2. 音響的に類似した単語の弁別

同音異義語（取り/撮り — 全3話者で共通ミス）や音響類似語（雨/姉）の混同。AMのGMM表現力の限界。

### 3. ~~学習データの話者・音源多様性の不足~~ → v11で改善

v11で55話者に拡張 (30→55)。test_tts 1/10→5/10、コーパス外3話者平均 47%→57%に改善。ただし全てTTS合成音声であり、実音声の声質・録音環境は模擬できない。

### 4. ~~感情・発話スタイル変動への音響モデル頑健性~~ → v8で解決

感情100%: 2/10 (v6) → **10/10 (v8)**。速度変換データ拡張によりピッチ・話速変動への耐性を獲得。

### 5. ~~自然言語テキストによるLM強化~~ → 実施済み

Wikipedia日本語ダンプ → `cmd/wikitext` + `cmd/lmtext` で辞書フィルタ → トライグラムLM構築 + デコーダ再結合拡張。コーパス外3話者平均が43% → 47%に微増。ただしWikipedia 4,173,332文から辞書フィルタ通過が393文のみ (採用率0.03%) — 辞書カバレッジがボトルネック。v16では辞書2,000語+新wikitext抽出 (14.8M文) で11,407文通過 (採用率0.08%) に改善。

## 改善方向

| 優先度 | 施策 | 期待効果 |
|---|---|---|
| ~~高~~ | ~~TTS話者追加~~ | ~~話者間汎化の向上~~ → **v11で実施: +10pt** (30→55話者) |
| ~~高~~ | ~~実音声データの追加学習~~ | ~~話者間汎化のさらなる向上~~ → **実施済み: リグレッション** (Common Voice 4,686発話混合でOOC 57%→35%。4-mix GMMの表現力限界) |
| ~~高~~ | ~~VTLN（声道長正規化）~~ | ~~話者正規化による混同低減~~ → **実施済み: 効果なし** (TTS音声の声道長差が小さいため) |
| ~~中~~ | ~~辞書拡張 + 混同フィルタ~~ | ~~語彙カバレッジ向上~~ → **実施済み: IC 20/20達成、OOC -2pt** (1,176→1,694語。混同フィルタでIC維持、OOCは微減) |
| ~~中~~ | ~~GMM混合数増加 (8-mix)~~ | ~~弁別力向上~~ → **実施済み: 過学習** (OOC 55%→38%。データ不足で7,296ガウス推定が不安定) |
| ~~中~~ | ~~DNN深層化 + Dropout~~ | ~~フレーム精度・汎化性能向上~~ → **v14で実施: OOC +5pt** (4層×512+DO 0.2。val_acc 69.9%→80.0%、OOC 45%→50%) |
| ~~中~~ | ~~Cosine LR Annealing~~ | ~~学習収束改善~~ → **v15で実施: OOC +10pt** (val_acc 80.0%→84.0%、OOC 50%→60%) |
| ~~中~~ | ~~Label Smoothing~~ | ~~汎化性能向上~~ → **不採用**: ε=0.05/0.1共にデコーダ認識不可。HMM-DNNハイブリッドではlogitのダイナミックレンジ低下が致命的 |
| ~~中~~ | ~~辞書5,000語拡張~~ | ~~外来語カバレッジ~~ → **パラメータ調整で解決**: MT=3000+WP=5で5,000語でも70/90 (旧49/90)。辞書サイズ自体は問題なし |
| ~~中~~ | ~~デコーダパラメータ最適化~~ | ~~語彙拡大耐性~~ → **実施済み: +10〜+21pt**。tunerグリッドサーチで最適WP/MT/LW探索。全語彙サイズで大幅改善 |
| 低 | コーパスのドメイン拡張（医療・IT等） | 未知文カバレッジ向上 |

## パフォーマンス最適化

### GMM: Apple Accelerate (cblas_dgemm) バッチ尤度計算 **2.1x**

対角共分散GMMのMahalanobis距離を2回の行列積に分解し、Apple Accelerate (`cblas_dgemm`, AMXコプロセッサ) で T×K 個の尤度を一括計算。非darwin環境では純Goフォールバック。

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| GMM LogProbBatch 300f×4mix | 71,000 ns | 34,000 ns | **2.1x** |

| # | 施策 | 状態 |
|---|---|---|
| 13 | `internal/blas/`: CGo Accelerate wrapper + 純Goフォールバック (ビルドタグ分離) | ✅ |
| 14 | `acoustic/gmm_batch.go`: Mahalanobis分解 → 2×dgemm + logsumexp のバッチ計算 | ✅ |

### 特徴量抽出 第2弾: FFT SIMDアセンブリ + フレーム並列化 **-72%** (3.6x)

FFTバタフライのsplit R/I化 + NEON/SSE2アセンブリ、および `runtime.NumCPU` goroutineによるフレーム並列化。

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| Extract_1sec | 591,000 ns | 306,000 ns | **-48%** |
| Extract_5sec | 2,950,000 ns | 1,264,000 ns | **-57%** |
| Extract_30sec | 17,430,000 ns | 4,840,000 ns | **-72% (3.6x)** |

| # | 施策 | 状態 |
|---|---|---|
| 11 | FFTバタフライをsplit R/Iレイアウトに変更、NEON (arm64) / SSE2 (amd64) アセンブリ | ✅ |
| 12 | Extract/ExtractPowerSpectraのフレームループをgoroutineワーカプール並列化 | ✅ |

### ~~特徴量抽出 第1弾~~ **-48%**, allocs **-99.3%** → 対応済み

| ベンチマーク | Before | After | 速度 | メモリ | allocs |
|---|---|---|---|---|---|
| Extract_1sec | 1,150,000 ns / 574 allocs | 597,000 ns / 104 allocs | **-48%** | **-50%** | **-82%** |
| Extract_5sec | 5,800,000 ns / 2,574 allocs | 3,020,000 ns / 104 allocs | **-48%** | **-58%** | **-96%** |
| Extract_30sec | 34,470,000 ns / 15,074 allocs | 17,807,000 ns / 104 allocs | **-48%** | **-61%** | **-99.3%** |

| # | 施策 | 状態 |
|---|---|---|
| 1 | Hamming窓係数テーブル事前計算 | ✅ |
| 2 | FFT twiddle factorテーブル事前計算 | ✅ |
| 3 | cepstraバッファ一括確保 | ✅ |
| 4 | deltaバッファ一括確保 | ✅ |
| 5 | bit-reversal permutationテーブル化 | ✅ |
| 6 | フレーム分割をslice viewに変更、窓適用をFFTバッファロードに融合 | ✅ |

### 学習: BaumWelch **-15%**

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| BaumWelch_10seq_50frames | 2,050,000 ns | 1,744,000 ns | **-15%** |

| # | 施策 | 状態 |
|---|---|---|
| 7 | E-step統計量蓄積のポインタチェイシング削減・乗算最適化 | ✅ |

### デコーダ: Decode **-17%**

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| Decode_5vocab_50frames | 1,044,000 ns | 909,000 ns | **-13%** |
| Decode_10vocab_100frames | 6,068,000 ns | 5,365,000 ns | **-12%** |
| Decode_20vocab_200frames | 44,122,000 ns | 36,428,000 ns | **-17%** |

| # | 施策 | 状態 |
|---|---|---|
| 8 | トークン再結合キーをstring→数値IDに変更 | ✅ |
| 9 | ビームプルーニングをsort.Slice→quickselectに変更 | ✅ |

### DNN学習: 並列サブバッチ処理 **2.2x**

ミニバッチ(256)を`runtime.NumCPU()`個のサブバッチに分割し、各ワーカーが独立にforward/backwardを実行。勾配を集約後にAdam更新を1回実行する方式。

#### 背景

バッチサイズ256のBLAS行列積は小さく、Apple Accelerate (AMX) のスループットを活かしきれない。CPUコア使用率が低い状態だった。

#### 手法

```
mega-batch (256 × NumCPU frames)
  ├─ worker 0: sub-batch (256 frames) → forward → backward → local grads
  ├─ worker 1: sub-batch (256 frames) → forward → backward → local grads
  ├─ ...
  └─ worker N: sub-batch (256 frames) → forward → backward → local grads
  → barrier (sync.WaitGroup)
  → gradient accumulation (addSlice)
  → Adam update (1回)
```

- 各ワーカーは独立な`dnnWorkspace`と`workerGrads`バッファを持つ (ロック不要)
- 勾配集約はシリアルだがO(パラメータ数)のみで軽量
- ワーカー数上限8 (メモリ増加とのトレードオフ)

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| DNN Training 100Kフレーム 1epoch (batch=256, M2) | 1,112 ms | 512 ms | **2.2x** |

| # | 施策 | 状態 |
|---|---|---|
| 15 | `acoustic/dnn_train.go`: mega-batch → NumCPU goroutine sub-batch並列 forward/backward + gradient accumulation | ✅ |

### 学習パイプライン: 全体 **-77%** (4.4x高速化)

| ベンチマーク | Before | After | 速度 |
|---|---|---|---|
| 全学習パイプライン (2,469発話, 5-way augment, triphone) | 4m33s | 1m02s | **-77%** |

| # | 施策 | 状態 |
|---|---|---|
| 10 | 特徴量抽出・モノフォン学習・強制アライメント・トライフォン学習をgoroutineワーカプール(NumCPU)で並列化 | ✅ |

## 現在の推奨構成

```
AM:   models/v15/am.gob (55話者 5,794発話, 5-way augment, 4-mix GMM, トライフォン)
DNN:  models/v15/dnn.gob (4層×512, Dropout 0.2, Cosine LR, 50エポック, val_acc 84.0%)
LM:   models/v15/lm.arpa (トライグラム、14,643文: テンプレート14,250 + Wikipedia393)
Dict: models/v15/dict.txt (1,694語、混同フィルタ済)

外来語対応版 (v16, -1文の微減で外来語51語を追加):
LM:   models/v16/lm.arpa (トライグラム、29,774文: テンプレート18,367 + Wikipedia11,407)
Dict: models/v16/dict.txt (2,000語 = v15 1,694語 + 外来語51 + fill 255)

認識コマンド (v15, 1,694語, チューニング済):
/tmp/transcript \
  -am models/v15/am.gob \
  -dnn models/v15/dnn.gob \
  -lm models/v15/lm.arpa \
  -dict models/v15/dict.txt \
  -lm-weight 11 -word-penalty 13 \
  -wav input.wav

認識コマンド (5,000語辞書):
/tmp/transcript \
  -am models/v15/am.gob \
  -dnn models/v15/dnn.gob \
  -lm data/work/lm_5000.arpa \
  -dict data/work/dict_5000.txt \
  -lm-weight 10 -word-penalty 5 -max-tokens 3000 \
  -wav input.wav

パラメータチューニングコマンド:
go run ./cmd/tuner \
  -am models/v15/am.gob -dnn models/v15/dnn.gob \
  -lm models/v15/lm.arpa -dict models/v15/dict.txt \
  -manifest "data/test_tts/manifest.tsv,..." \
  -lm-weights "8,10,12,15" -word-penalties "0,5,10,15"

DNN学習コマンド:
go run ./cmd/dnntrain \
  -manifest data/manifest_all_v5.tsv \
  -dict data/work/dict_2000_filtered.txt \
  -am data/work/am_tri_8mix.gob \
  -output models/v15/dnn.gob \
  -hidden 512 -layers 4 -dropout 0.2 -lr-schedule cosine -epochs 50 -patience 0 -augment

GMM学習コマンド:
go run ./cmd/train \
  -manifest data/manifest_all_v5.tsv \
  -dict data/work/dict_lm_v8.txt \
  -output data/work/am_tri_v11.gob \
  -mix 4 -iter 20 -align-iter 2 -triphone -augment

LM構築コマンド:
go run ./cmd/lmbuild -order 3 -output models/v14/lm.arpa \
  training/corpus8_expanded.txt data/work/nlp_corpus.txt
```
